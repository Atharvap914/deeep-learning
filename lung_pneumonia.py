# -*- coding: utf-8 -*-
"""lung_pneumonia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bCsZrHXKDZHhm2uiBMuOzN3dM5Cfm6Mt
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Specify the path to your zip file
zip_file_path = '/content/drive/My Drive/chest_xray.zip' # Make sure this is the correct path to your zip file

# Specify the directory where you want to extract the contents
extracted_path = '/tmp/chest_xray_extracted' # You can choose any directory path

# Create the extraction directory if it doesn't exist
os.makedirs(extracted_path, exist_ok=True)

# Unzip the file
if os.path.exists(zip_file_path):
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extracted_path)
    print(f"Successfully extracted {zip_file_path} to {extracted_path}")

    # List the contents of the extracted directory to verify
    print(f"\nContents of the extracted directory '{extracted_path}':")
    for filename in os.listdir(extracted_path):
        print(filename)
else:
    print(f"Zip file not found at: {zip_file_path}")

"""# Task
Create a multi-hybrid model for image classification using the images in the directory "extracted_images".

## Data loading and exploration

### Subtask:
Load the images from the extracted directory and explore the dataset structure and image properties (e.g., size, format, number of classes).

**Reasoning**:
The previous code block successfully extracted the zip file to `/tmp/chest_xray_extracted`. The next step is to explore the extracted directory to understand the dataset structure, including the number of classes and images per class. I will iterate through the subdirectories and count the images.
"""

# Specify the path to the extracted dataset directory
extracted_dataset_path = '/tmp/chest_xray_extracted/chest_xray' # Based on the previous output, the actual data is in a subfolder

# List the contents of the main extracted directory
print(f"Contents of the dataset directory '{extracted_dataset_path}':")
dataset_contents = os.listdir(extracted_dataset_path)
print(dataset_contents)

# Initialize a dictionary to store image counts per class
class_image_counts = {}
total_images = 0
sample_images = {}

# Iterate through the subdirectories (presumably representing different classes)
print("\nExploring class directories:")
for class_name in dataset_contents:
    class_dir_path = os.path.join(extracted_dataset_path, class_name)

    # Check if the item is a directory
    if os.path.isdir(class_dir_path):
        print(f"  Class directory: {class_name}")
        try:
            # List the files (images) within the class directory
            image_files = [f for f in os.listdir(class_dir_path) if os.path.isfile(os.path.join(class_dir_path, f))]

            # Count the number of images in the subdirectory
            num_images = len(image_files)
            class_image_counts[class_name] = num_images
            total_images += num_images

            # Select a few sample images from this class
            if num_images > 0:
                sample_images[class_name] = image_files[:3] # Take up to the first 3 images

        except Exception as e:
            print(f"    Error listing files in {class_dir_path}: {e}")
    else:
        print(f"  Skipping non-directory item: {class_name}")


# Print the total number of images and the number of images per class
print("\nDataset Summary:")
print(f"Total number of images: {total_images}")
print("Number of images per class:")
for class_name, count in class_image_counts.items():
    print(f"  {class_name}: {count}")

# Print sample image filenames
print("\nSample image filenames per class:")
for class_name, filenames in sample_images.items():
    print(f"  {class_name}: {filenames}")

"""**Reasoning**:
The previous step showed that the actual image data is likely within subdirectories like 'train', 'test', and 'val' inside the '/tmp/chest_xray_extracted/chest_xray' folder, but the counts were zero for train, test, and val, and there were unexpected files like `.DS_Store` and `._chest_xray`. This suggests there's another level of directories or the previous listing was incomplete. I need to explore the subdirectories within 'train', 'test', and 'val' to find the actual class folders (like 'NORMAL' and 'PNEUMONIA') and count the images there. I will also examine the image properties of a sample image.


"""

from PIL import Image
import numpy as np

# Define the main dataset directory
main_dataset_dir = '/tmp/chest_xray_extracted/chest_xray'

# Initialize dictionaries for counts and sample images
class_image_counts_per_split = {}
sample_images_per_split = {}
total_images_in_splits = 0

# Define the expected data splits
data_splits = ['train', 'test', 'val']

print(f"Exploring data splits within '{main_dataset_dir}':")

for split in data_splits:
    split_dir_path = os.path.join(main_dataset_dir, split)
    class_image_counts_per_split[split] = {}
    sample_images_per_split[split] = {}

    if os.path.isdir(split_dir_path):
        print(f"\n  Exploring split: {split}")
        split_contents = os.listdir(split_dir_path)
        print(f"    Contents of '{split_dir_path}': {split_contents}")

        for class_name in split_contents:
            class_dir_path = os.path.join(split_dir_path, class_name)

            # Check if the item is a directory and not a hidden directory like __MACOSX
            if os.path.isdir(class_dir_path) and not class_name.startswith('__'):
                print(f"    Class directory within {split}: {class_name}")
                try:
                    # List the files (images) within the class directory
                    image_files = [f for f in os.listdir(class_dir_path) if os.path.isfile(os.path.join(class_dir_path, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]

                    # Count the number of images
                    num_images = len(image_files)
                    class_image_counts_per_split[split][class_name] = num_images
                    total_images_in_splits += num_images

                    # Select a few sample images from this class
                    if num_images > 0:
                        sample_images_per_split[split][class_name] = image_files[:3] # Take up to the first 3 images

                        # Examine properties of a sample image (only for the first encountered image)
                        if 'sample_image_properties' not in locals():
                            sample_image_path = os.path.join(class_dir_path, image_files[0])
                            try:
                                with Image.open(sample_image_path) as img:
                                    sample_image_properties = {
                                        'path': sample_image_path,
                                        'format': img.format,
                                        'mode': img.mode,
                                        'size': img.size,
                                        'class': class_name,
                                        'split': split
                                    }
                                print(f"      Sample image properties from '{sample_image_path}': {sample_image_properties}")
                            except Exception as img_e:
                                print(f"      Error opening sample image {sample_image_path}: {img_e}")


                except Exception as e:
                    print(f"      Error listing files in {class_dir_path}: {e}")
            else:
                 if not class_name.startswith('__'): # Avoid printing for __MACOSX again
                    print(f"    Skipping non-directory or hidden item in {split}: {class_name}")

    else:
        print(f"  Split directory not found: {split_dir_path}")


# Print the dataset structure and counts
print("\nDataset Structure and Summary:")
print(f"Total number of images across train, test, and val splits: {total_images_in_splits}")
print("\nImage counts per class per split:")
for split, class_counts in class_image_counts_per_split.items():
    print(f"  {split.upper()} Split:")
    if class_counts:
        for class_name, count in class_counts.items():
            print(f"    {class_name}: {count}")
    else:
        print("    No image directories found.")

# Print sample image filenames
print("\nSample image filenames per class per split:")
for split, class_samples in sample_images_per_split.items():
    print(f"  {split.upper()} Split:")
    if class_samples:
        for class_name, filenames in class_samples.items():
            print(f"    {class_name}: {filenames}")
    else:
        print("    No sample images found.")

# Print the properties of the examined sample image
if 'sample_image_properties' in locals():
    print("\nProperties of a sample image:")
    print(f"  Path: {sample_image_properties['path']}")
    print(f"  Format: {sample_image_properties['format']}")
    print(f"  Mode: {sample_image_properties['mode']}")
    print(f"  Size: {sample_image_properties['size']}")
    print(f"  Class: {sample_image_properties['class']}")
    print(f"  Split: {sample_image_properties['split']}")
else:
    print("\nNo sample image properties could be determined.")

"""## Data preprocessing

### Subtask:
Prepare the images for model training, which may include resizing, normalization, and data augmentation.

**Reasoning**:
Use Keras ImageDataGenerator to create data generators for training, validation, and test sets. This will handle resizing, normalization, and data augmentation.
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define target image dimensions
IMG_HEIGHT = 150
IMG_WIDTH = 150
IMAGE_SIZE = (IMG_WIDTH, IMG_HEIGHT)
BATCH_SIZE = 32

# Define the main dataset directory
main_dataset_dir = '/tmp/chest_xray_extracted/chest_xray'
train_dir = os.path.join(main_dataset_dir, 'train')
validation_dir = os.path.join(main_dataset_dir, 'val')
test_dir = os.path.join(main_dataset_dir, 'test')


# Create data generators with augmentation and normalization
# Rescale pixel values to [0, 1]
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Only rescale for validation and test sets (no augmentation)
validation_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Flow images from directories
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary' # Since we have two classes (NORMAL, PNEUMONIA)
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary'
)

print("Train generator class indices:", train_generator.class_indices)
print("Validation generator class indices:", validation_generator.class_indices)
print("Test generator class indices:", test_generator.class_indices)

"""## Model architecture design

### Subtask:
Define the architecture of your multi-hybrid model. This could involve combining different types of layers or pre-trained models.

**Reasoning**:
Import the necessary libraries for building a hybrid model, including Keras layers and a pre-trained base like VGG16.
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, concatenate
from tensorflow.keras.applications import VGG16

"""**Reasoning**:
Define the architecture of the multi-hybrid model, combining a VGG16 convolutional base with custom layers for binary classification.


"""

# Define the input shape
input_shape = (IMG_HEIGHT, IMG_WIDTH, 3) # Assuming images are RGB

# Load the VGG16 model with pre-trained weights, excluding the top classification layer
vgg = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

# Freeze the convolutional base to prevent weights from being updated during training
vgg.trainable = False

# Create a custom model on top of the VGG16 base
input_tensor = Input(shape=input_shape)

# Use the VGG16 model as the convolutional base
x = vgg(input_tensor, training=False) # Set training=False to keep VGG16 layers in inference mode

# Add custom layers for classification
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
output_tensor = Dense(1, activation='sigmoid')(x) # Binary classification

# Create the hybrid model
model = Model(inputs=input_tensor, outputs=output_tensor)

# Print the model summary
model.summary()

"""## Model compilation

### Subtask:
Configure the model for training by specifying the optimizer, loss function, and metrics.

**Reasoning**:
Import necessary components from tensorflow.keras and compile the model with the specified optimizer, loss function, and metrics.
"""

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Accuracy

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Print the model configuration after compilation to verify
print("Model compiled successfully.")
print("Optimizer:", model.optimizer.get_config()['name'])
print("Loss function:", model.loss)
print("Metrics:", [metric.name for metric in model.metrics])

"""## Model training

### Subtask:
Train the model on your prepared dataset.

**Reasoning**:
Train the compiled model using the prepared data generators and store the training history.
"""

import time

# Define the number of training epochs
EPOCHS = 3

print(f"Starting model training for {EPOCHS} epochs...")
start_time = time.time()

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // BATCH_SIZE
)

end_time = time.time()
print(f"Model training finished after {end_time - start_time:.2f} seconds.")

# The training history is now stored in the 'history' variable.

"""## Model Evaluation

### Subtask:
Evaluate the trained model's performance using appropriate metrics.

**Reasoning**:
Evaluate the trained model on the test dataset using the `evaluate` method and print the results.
"""

# Evaluate the model on the test set
print("Evaluating the model on the test dataset...")
test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // BATCH_SIZE)




"""## Save the trained model

### Subtask:
Save your trained Keras model to a file so it can be loaded in the Streamlit application.

**Reasoning**:
Save the trained Keras model to a file.
"""

import os

# Define the directory path where you want to save the model
# Using the current working directory
save_dir = '.'

# Construct the full path for the model file
model_filename = 'multi_hybrid_model.h5'
model_path = os.path.join(save_dir, model_filename)

# Save the trained model
model.save(model_path)

